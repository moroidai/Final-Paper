---
title: "What makes teams strong?"
author: "Dai Moroi"
date: "April 6th 2022"
output:
  bookdown::pdf_document2:
    toc: yes
abstract: "This paper analyzes the basic statistics of 2018-19 season England proffesional soccer league, Premier League, to figure out what makes the team strong, and also examines cause and effect of home advantage. The result indicates that there are storng correlations between strength of the teams and average possessions, the number of shots, goals scored, goals conceded, and the percentage of goal scored out of total number of shots. As for home advantage, although storong home advantage is observed, its cause is not explained in terms of the crowd factors.The linear regression model is used to analyze the data.  \\par \\textbf{Keywords:} football, soccer, home advantage, linear regression, Premier League"
thanks: 'Code and data are available at: [github.com/moroidai/Final-Paper](https://github.com/moroidai/Final-Paper).'
subtitle: "Exploring statistics of Premier League 2018/19"
bibliography: references.bib
---

```{r setup, include=FALSE, warning=FALSE}

if (!require("bookdown")) install.packages("bookdown")
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("patchwork")) install.packages("patchwork")
if (!require("ggpubr")) install.packages("ggpubr")
if (!require("knitr")) install.packages("knitr")
library(tidyverse)
library(bookdown)
library(ggpubr)
library(patchwork)
library(knitr)

```


```{r data,include=FALSE, warning=FALSE}
## Be sure to run 01-gather_data.R and 02-clean_and_prepare_data.R before ##
data <-
  read_csv(file = "raw_data.csv",
           show_col_types = FALSE)

data_2 <-
  read_csv(file = "raw_data_2.csv",
           show_col_types = FALSE)
```

# Introduction

What makes the team strong? What kind of characteristics does a strong team have? To answer these questions with convincing stories, statistics can play a essential role. Since the Internet enabled us to accumulate data easily, use of data has been featured in the world of sports, including soccer. This paper takes the example of 2018-19 season of England professional soccer league, Premier League, and examines its basic statistics, such as goals, possession rate, the number of shots, etc., from various aspects.

First, I explore the relationship between the number of wins and a variety of factors so that we can better understand the features of the teams winning many times. Then, the focus will be moved to the next topic, home advantage. In the sports competition, it often occurs that weak teams defeat strong teams at their home stadium. This phenomenon is called home advantage, which is defined by Courneya and Carron [@citehomeadvantage] as "the consistent finding that home teams in sports competitions win over 50% of the games played under a balanced home and away schedule". I see if the home advantage really exists in the Premier League, and then try to discover its cause from crowd factors. Thirdly, I focus on the statistics of the "Big six" Premier league teams, that is Manchester United, Liverpool, Chelsea, Arsenal, Manchester City and Tottenham Hotspur. These six teams are said to be biggest and most successful clubs in the league, and they actually all finished as top 6 teams in 2018-19 season. Looking at the similarity of these six teams and differences between them and others should be useful in learning the characteristics of strong teams.


The remainder of this paper is structured as follows: Section 2 discusses the data using graphs and tables. This section also explains the methodology that the data was collected and bias they have. Section 3 discusses the model, Section 4 presents the results, and finally Section 5 discusses the findings and weaknesses to be overcome. All the analysis is done in a reproducible way.



# Data

## Data and variables


The analysis for this paper uses the R statistical programming language [@citeR], primarily using `tidyverse` packages [@citetidyverse] for data manipulation. It uses `pdftools` package for parsing the PDF [@citepdftools], and `bookdown` package [@citebookdown], `ggpubr` package [@citeggpubr], `patchwork` package [@citepatchwork], and `knitr` package [@citeknitr] for making a R markdown report.

The data, in a .csv format, are available on the website, [FootyStats](https://footystats.org/). Please note that the data of Premier League 2018-19 season is the only free data right now. The data contains 20 observations of 735 variables in total. Although there are many variables, including corners, cards, offside, and data divided by every 15 minutes, this paper focuses on main stats, such as the number of goals scored, wins, or average possessions. All the variables and its explanation are available [here](https://footystats.org/download-stats-csv). Table \@ref(tab:example) shows the he first ten rows and five columns.


```{r example,fig.width = 5, fig.height = 3, echo = FALSE, message = FALSE, warning = FALSE, fig.align = "center"}

data[1:10,1:5] |>
kable(caption = "First ten rows and five columns of the dataset",
      booktabs = TRUE, 
    linesep = "")
```

## Methodology

The dataset is based on the official open statistics by premierleague.com. Official Premier League performance data is collected and analysed by Opta, part of Stats Perform. The methodologies of data collections is described as follows [@citepremierleague]: "Live data is collected by a three-person team covering each match. Two highly trained analysts use a proprietary video-based collection system to gather information on what happens every time a player touches the ball, which player it was and where on the pitch the action occurred. Alongside them, a quality control analyst has the ability to rewind the video feed frame-by-frame in order to make certain that the information being distributed is as precise and consistent as possible. All the Premier League data collected is then subject to an exhaustive post-match check to ensure accuracy". Thus, the process of collecting data should be appropriate and trustworthy.

Because most of the data are clearly observed and counted by trained analysts, there is little bias in this dataset. However, there are two possible bias. The first possible bias is that some data are sometimes hard to count and can lead to overcounting or undercounting. For example, when you tell whether a shot is on target or off target, it sometimes happens that the shot is deflected from another team player. In this case, the shot is counted based on if the shot would have been on target or off target without deflection, but it is often hard to distinguish whether the original shot was on target or not. Yet, these sort of thing rarely happens, so it does not really affect the outcome of the analysis. The second possible bias is about the data called expected goals. Expected goals is a measure created recently to assess the quality of shots at goal. Before the invention of the concept of expected goals, we could just know the number of shots or chances created but not the quality of chances. Even though they are counted the same as shots, one can be better chance and another can be not a good chance. Here, expected goals try to measure the probability that shots will be scored based on many factors. Those factors include distance from goal, shot angle, type of attack, body part used to shoot, and assist type. I don't show the exact calculation method here because it is not very relevant to the paper, but I can say that the data of expected goal can be biased because of its complexity of calculation. 



# Model

This paper uses simple linear reggression model for the analysis. The mathematical equation of this model can be expressed as follows:

$$
y = \beta_{1}+\beta_{2}x+\epsilon
$$

Where $\beta_{1}$ is the intercept and $\beta_{2}$ is the slope. $\epsilon$ is residuals, the part of Y the regression model is unable to explain.



# Results

## General analysis

Firstly, using linear regression, I look at the correlations between the number of wins and four basic stats, that is goals scored, goals conceded, average possession rates, and the number of shots. Figure \@ref(fig:goalsandwins) shows the regression line and the correlations between wins and the four factors.
You can see that there are very strong positive correlation between wins and goals scored, and strong negative correlation between wins and goals conceded, where correlation coefficients are 0.97 and -0.91 respectively. In addition, it is noted that the correlations are also very strong with average possession and the number of shots, where correlation coefficients are 0.84 and 0.86.

From this result, we can conclude that the strong teams scores many goals, while they rarely concede it. Also, they keep high possession rate throughout matches and shoot many times.

```{r goalsandwins, fig.cap = "Correlations between wins and four factors", fig.width = 10, fig.height = 6, echo = FALSE, message = FALSE, warning = FALSE, fig.align = "center"}

goals_scored <-
data |>
  ggscatter(x = "goals_scored", y = "wins", 
            add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Goals scored", ylab = "Nuber of wins") +
  theme_minimal()

goals_conceded <-
data |>
  ggscatter(x = "goals_conceded", y = "wins", 
            add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Goals conceded", ylab = "Nuber of wins") +
  theme_minimal()

possession <-
data |>
  ggscatter(x = "average_possession", y = "wins", 
            add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Average possesion(%)", ylab = "Nuber of wins") +
  theme_minimal()

shots <-
data |>
  ggscatter(x = "shots", y = "wins", 
            add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Number of shots", ylab = "Nuber of wins") +
  theme_minimal()

(goals_scored | goals_conceded) / (possession | shots)

```


## Home advantage

As stated in the Introduction, there is a concept called home advantage. Let's see if we can observe home advantage in Premier League. Table \@ref(tab:homeadvantage) shows the winning percentages of all teams in Premier League 2018-19 when they play at home stadium or away stadium, and the difference between the two. Although there is a exception of Crystal Palace FC, where they are 20% more likely to win at away, almost all teams have significant home advantage, where the average of difference in winning percentage is over 15%. According to the definition of home advantage, home team should win over 50% of matches, but the results still shows that there is noticable home advantage in Premier League.

```{r homeadvantage, fig.width = 5, fig.height = 3, echo = FALSE, message = FALSE, warning = FALSE, fig.align = "center"}

## create new column about the percentage of win at home and away ##

data <-
  data |>
  mutate(home_winning_percentage =  wins_home / matches_played_home *100,
         away_winning_percentage = wins_away / matches_played_away *100,
         difference = home_winning_percentage - away_winning_percentage)

table <-
  data |>
  select(team_name,
         home_winning_percentage,
         away_winning_percentage,
         difference)

table |>
  arrange(desc(home_winning_percentage)) |>
  kable(caption = "Home advantage of Premier League 2018/19",
      booktabs = TRUE, 
    linesep = "",
    col.names = c("Team name",
                  "Win at home (%)",
                  "Win at away (%)",
                  "Difference"),
    digits = 1)

```

But what causes home advantage? You probably think this phenomenon is strange because normally stronger team should win no matter where they play. To figure out the reasons behind it, I focus on the crowd factors. My idea is that enthusiastic atmosphere that a huge crowd makes cheers players, which lead to make the most of their ability. If that is true, the number of crowds being at their home stadium matters. Figure \@ref(fig:homeadvantageattended) shows the correlation between the number of fans at stadiums and home advantage measured by winning percentage difference. The figure indicates that we can not recognize the correlation because the correlation coefficient is only 0.23.


```{r homeadvantageattended, fig.cap = "Correlations between home advantage and number of fans at stadiums", fig.width = 5, fig.height = 3, echo = FALSE, message = FALSE, warning = FALSE, fig.align = "center"}

data <-
  data |>
  mutate(average_attendance_home = data_2$average_attendance_home)

data |>
  ggscatter(x = "average_attendance_home", y = "difference", 
            add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Average attendance at home stadium", ylab = "Home advantage by difference(%)")  +theme_minimal()


```


But the problem of using the raw number of attendance at stadium is that we could underestimate the power of full stadium. For example, a full stadium with 20000 fans can be more enthusiastic than a stadium with 20000 fans whose capacity is 60000. In order to take this effect into account, I use crowd density as a factor, which is calculated by dividing the average attendance by stadiums' capacity. Figure \@ref(fig:crowd) shows the correlation between home advantage and crowd densities. However, the results still don't show that there is positive correlation between home advantage and crowd factors. 

```{r crowd, fig.cap = "Correlation between home advantage and crowd densities", fig.width = 5, fig.height = 3, echo = FALSE, message = FALSE, warning = FALSE, fig.align = "center"}

data <-
  data |>
  mutate(stadium_capacities = c(60260, 62062, 55017, 32273, 26074, 39221, 21944, 32384, 11329, 74879, 53394, 40853, 60000, 20400, 52354, 33316, 25700, 30666, 24500, 32050),
         crowd_densities = data_2$average_attendance_home / stadium_capacities)

data |>
  ggscatter(x = "crowd_densities", y = "difference", 
            add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Average crowd density", ylab = "Home advantage by difference(%)") +theme_minimal()



```


## BIG 6

Here, I look at the characteristics of "Big six" teams, namely Manchester United, Liverpool, Chelsea, Arsenal, Manchester City and Tottenham Hotspur. 

Figure \@ref(fig:top6) shows the relation between goals scored and expected goals, and blue dots shows the data of Big six teams. The 45 degree line means that expected goals equals the actual goals scored, which is supposed to be most expected to happen. You can see that all big six teams are located higher than the line, which means they scored more than expected. This happens probably because they have a lot of good strikers and those strikers are good at shooting, which resulted in scoring more than expected.  

```{r top6, fig.cap = "The relation between goals scored and expected goals, sorted by BIG6", fig.width = 5, fig.height = 3, echo = FALSE, message = FALSE, warning = FALSE, fig.align = "center"}
data <-
  data |>
  mutate(top6 = if_else(league_position %in% c(1:6), "YES", "NO"))


data |>
ggplot(aes(x = xg_for_avg_overall, y = goals_scored_per_match, color = top6)) +
  geom_point() +
  theme_minimal() +
  geom_abline(intercept = 0, slope = 1, size = 0.5, color = "grey") +
    xlab("expected goals per match") +
  ylab("Goals scored per match")
```

Same as the previous figure, when you look at Figure \@ref(fig:top62), which shows Relations between the league position and percentage of goals scored out of total shots, it is noted that "Big six" teams are more likely to score when they shoot. 


```{r top62, fig.cap = "Relations between the league position and percentage of goals scored out of total shots, sorted by BIG6", fig.width = 5, fig.height = 3, echo = FALSE, message = FALSE, warning = FALSE, fig.align = "center"}

data <-
  data |>
  mutate(percentage_of_goals_scored_out_of_shots = goals_scored / shots *100)


data |>
ggplot(aes(x = league_position, y = percentage_of_goals_scored_out_of_shots, color = top6)) +
  geom_point() +
  theme_minimal() +
  xlab("League posision") +
  ylab("Percentage of goals scored out of total shots")

```




# Discussion

Over the analysis above, I explore the basic stats to find the characteristics of strong teams and what makes them strong. I would like to discuss two points regarding the results here. First point is about the fact that the strong correlation between wins and the average possession rate is observed. I think that this result can change over time because the trend tactics of soccer have been changing, and I expect that this correlation will decay over time. Almost 10 years ago, especially in Spanish league, strong teams preferred keeping possession, but now it is popular that they don't stick to keep possession, but instead bring the ball straight faster as possible to the goal, because hey can attack before the opponent's defender get ready if they attack fast. The second point is about crowd density. Looking at Figure \@ref(fig:crowd), you probably notice that the crowd densities is pretty high in Premier league, which means all the teams played in the full stadiums, which makes it difficult to analyze home advantage based on crowd factors. Therefore, I can say that Premier League is not a good example for analyzing crowd factors because it is very popular. Leagues with less popularity should be chosen, if you want to do it.

The limitation of this paper is that it only uses the data of single season and single league. As a next step, it is recommended to compare it over time or across different leagues. Also, the dataset only contains basic stats, so using more advanced and deeper stats, such as the one using graphic data of fields, is highly recommended.

\appendix


# Appendix {-}

# Datasheet

**Motivation**

1. *For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.*
    - There is no description as to why they created the dataset on the website.
2. *Who created the dataset (for example, which team, research group) and on behalf of which entity (for example, company, institution, organization)?*
    - The dataset was created by the Footystats (https://footystats.org/). It is based on Premier League official stats.
3. *Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.*
    - There is no description.
4. *Any other comments?*
    - None

**Composition**

1. *What do the instances that comprise the dataset represent (for example, documents, photos, people, countries)? Are there multiple types of instances (for example, movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.*
	- The instances represent the Premier league teams.
2. *How many instances are there in total (of each type, if appropriate)?*
	- 20.
3. *Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (for example, geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (for example, to cover a more diverse range of instances, because instances were withheld or unavailable).*
	- Yes, it contains all possible instances
4. *What data does each instance consist of? "Raw" data (for example, unprocessed text or images) or features? In either case, please provide a description.*
	- Each instance consists of 735 continuous variables (raw data).
5. *Is there a label or target associated with each instance? If so, please provide a description.*
	- No.
6. *Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (for example, because it was unavailable). This does not include intentionally removed information, but might include, for example, redacted text.*
	- No
7. *Are relationships between individual instances made explicit (for example, users' movie ratings, social network links)? If so, please describe how these relationships are made explicit.*
	- No.
8. *Are there recommended data splits (for example, training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.*
	- There are no recommended data splits.
9. *Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.*
	- There are no errors, sources of noise, or redundancies in the dataset.
10. *Is the dataset self-contained, or does it link to or otherwise rely on external resources (for example, websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (that is, including the external resources as they existed at the time the dataset was created); c) are there any restrictions (for example, licenses, fees) associated with any of the external resources that might apply to a dataset consumer? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.*
	- The dataset is self-contained.
11. *Does the dataset contain data that might be considered confidential (for example, data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)? If so, please provide a description.*
	-  There is no confidential data, and the dataset is publicly available.
12. *Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why.*
	- It doesn't contain data that might be offensive, insulting, threatening, or might otherwise cause anxiety.
13. *Does the dataset identify any sub-populations (for example, by age, gender)? If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset.*
	- No.
14. *Is it possible to identify individuals (that is, one or more natural persons), either directly or indirectly (that is, in combination with other data) from the dataset? If so, please describe how.*
	- It is not possible to identify individuals.
15. *Does the dataset contain data that might be considered sensitive in any way (for example, data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description.*
	- No, it doesn't.
16. *Any other comments?*
	- None

**Collection process**

1. *How was the data associated with each instance acquired? Was the data directly observable (for example, raw text, movie ratings), reported by subjects (for example, survey responses), or indirectly inferred/derived from other data (for example, part-of-speech tags, model-based guesses for age or language)? If the data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.*
	- 
2. *What mechanisms or procedures were used to collect the data (for example, hardware apparatuses or sensors, manual human curation, software programs, software APIs)? How were these mechanisms or procedures validated?*
	- 
3. *If the dataset is a sample from a larger set, what was the sampling strategy (for example, deterministic, probabilistic with specific sampling probabilities)?*
	- 
4. *Who was involved in the data collection process (for example, students, crowdworkers, contractors) and how were they compensated (for example, how much were crowdworkers paid)?*
	- 
5. *Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (for example, recent crawl of old news articles)? If not, pleae describe the timeframe in which the data associated with the instances was created.*
	- 
6. *Were any ethical review processes conducted (for example, by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.*
	- 
7. *Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (for example, websites)?*
	- 
8. *Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.*
	- 
9. *Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.*
	- 
10. *If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).*
	- 
11. *Has an analysis of the potential impact of the dataset and its use on data subjects (for example, a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.*
	- 
12. *Any other comments?*
	- 

**Preprocessing/cleaning/labeling**

1. *Was any preprocessing/cleaning/labeling of the data done (for example, discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remaining questions in this section.*
	- 
2. *Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (for example, to support unanticipated future uses)? If so, please provide a link or other access point to the "raw" data.*
	- 
3. *Is the software that was used to preprocess/clean/label the data available? If so, please provide a link or other access point.*
	- 
4. *Any other comments?*
	- 

**Uses**

1. *Has the dataset been used for any tasks already? If so, please provide a description.*
	- No.
2. *Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.*
	- https://github.com/moroidai/paper4/blob/main/outputs/paper/paper.Rmd
3. *What (other) tasks could the dataset be used for?*
	- Analysis on soccer tactics, league characteristics, etc.
4. *Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a dataset consumer might need to know to avoid uses that could result in unfair treatment of individuals or groups (for example, stereotyping, quality of service issues) or other risks or harms (for example, legal risks, financial harms)? If so, please provide a description. Is there anything a dataset consumer could do to mitigate these risks or harms?*
	- No.
5. *Are there tasks for which the dataset should not be used? If so, please provide a description.*
	- No.
6. *Any other comments?*
	- None

**Distribution**

1. *Will the dataset be distributed to third parties outside of the entity (for example, company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.*
	- TBD
2. *How will the dataset be distributed (for example, tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?*
	- TBD
3. *When will the dataset be distributed?*
	- TBD
4. *Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/ or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.*
	- TBD
5. *Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.*
	- TBD
6. *Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.*
	- TBD
7. *Any other comments?*
	- TBD

**Maintenance**

1. *Who will be supporting/hosting/maintaining the dataset?*
	- TBD
2. *How can the owner/curator/manager of the dataset be contacted (for example, email address)?*
	- TBD
3. *Is there an erratum? If so, please provide a link or other access point.*
	- TBD
4. *Will the dataset be updated (for example, to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to dataset consumers (for example, mailing list, GitHub)?*
	- TBD
5. *If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (for example, were the individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.*
	- TBD
6. *Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers.*
	- TBD
7. *If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so, please provide a description.*
	- TBD
8. *Any other comments?*
	- TBD




# References

